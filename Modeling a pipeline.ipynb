{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aad4656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T01:36:31.853635Z",
     "start_time": "2021-09-18T01:36:31.845656Z"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2784f17a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:29:26.601148Z",
     "start_time": "2021-09-17T23:29:26.572222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from plotly import __version__\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import seaborn as sns\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "import nltk\n",
    "import wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag_sents, pos_tag, sent_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Import the SnowballStemmer to perform stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import *\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "import random\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37e65f",
   "metadata": {},
   "source": [
    "# Import Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da8e7054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:29:28.212991Z",
     "start_time": "2021-09-17T23:29:28.149514Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb9707a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:29:28.790691Z",
     "start_time": "2021-09-17T23:29:28.778723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Debugging</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To explain transitivity, let us look first at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>https://www.africanstorybook.org/#</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>Milka and John are playing in the garden. Her ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    url_legal       license  \\\n",
       "0  c0f722661                                          NaN           NaN   \n",
       "1  f0953f0a5                                          NaN           NaN   \n",
       "2  0df072751                                          NaN           NaN   \n",
       "3  04caf4e0c  https://en.wikipedia.org/wiki/Cell_division  CC BY-SA 3.0   \n",
       "4  0e63f8bea      https://en.wikipedia.org/wiki/Debugging  CC BY-SA 3.0   \n",
       "5  12537fe78                                          NaN           NaN   \n",
       "6  965e592c0           https://www.africanstorybook.org/#     CC BY 4.0   \n",
       "\n",
       "                                             excerpt  \n",
       "0  My hope lay in Jack's promise that he would ke...  \n",
       "1  Dotty continued to go to Mrs. Gray's every nig...  \n",
       "2  It was a bright and cheerful scene that greete...  \n",
       "3  Cell division is the process by which a parent...  \n",
       "4  Debugging is the process of finding and resolv...  \n",
       "5  To explain transitivity, let us look first at ...  \n",
       "6  Milka and John are playing in the garden. Her ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef50575",
   "metadata": {},
   "source": [
    "# Feature Engineering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1025aa1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:29:30.919366Z",
     "start_time": "2021-09-17T23:29:30.875361Z"
    },
    "code_folding": [
     12
    ]
   },
   "outputs": [],
   "source": [
    "def feature_engineering(\n",
    "    df='dataframe',\n",
    "    text_column='excerpt',\n",
    "    target_column='target',\n",
    "    get_sent_lengths=True,\n",
    "    get_word_lengths=True,\n",
    "    get_pos_counts=True,\n",
    "    top_n_word_count=True,\n",
    "    list_of_pos=['nouns', 'verbs', 'adjectives'],\n",
    "    model_path='C:\\\\Users\\\\abhil\\\\Documents\\\\DC\\\\Projects\\\\CommonLit Readability Prize\\\\tsdae-model',\n",
    "    model_name='tsdae-model',\n",
    "    directory='C:\\\\Users\\\\abhil\\\\Documents\\\\DC\\\\Projects\\\\CommonLit Readability Prize\\\\deploy\\\\'\n",
    "):\n",
    "    # Access the text\n",
    "    excerpts = df[text_column]\n",
    "\n",
    "    # Lets clean the strings\n",
    "    def clean_text(string):\n",
    "        string = pd.Series(string)\n",
    "        # remove white space and lowercase words\n",
    "        string = string.apply(str.strip).apply(str.lower)\n",
    "        # remove '\\n'\n",
    "        string = string.map(lambda x: re.sub('\\\\n', ' ', str(x)))\n",
    "        # remove punctuations\n",
    "        string = string.map(lambda x: re.sub(r\"[^\\w\\s]\", '', str(x)))\n",
    "\n",
    "        return string\n",
    "\n",
    "    excerpts = clean_text(excerpts)\n",
    "    print('Step 1: Text has been cleaned')\n",
    "\n",
    "    # Create an English language SnowballStemmer object\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    # Defining a function to perform both stemming and tokenization\n",
    "    def tokenize_and_stem(text):\n",
    "\n",
    "        # Tokenize by sentence, then by word\n",
    "        tokens = [y for x in sent_tokenize(text) for y in word_tokenize(x)]\n",
    "\n",
    "        # Filter out raw tokens to remove noise\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens if re.search('[a-zA-Z]', token)\n",
    "        ]\n",
    "        # Stem the filtered_tokens\n",
    "        stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "        # Remove stopwords\n",
    "        cleaned = [x for x in stems if x not in stopwords.words('english')]\n",
    "\n",
    "        # Join the cleaned tokens together\n",
    "        joined = ' '.join(cleaned)\n",
    "\n",
    "        return joined\n",
    "\n",
    "    print(\n",
    "        'Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...'\n",
    "    )\n",
    "\n",
    "    tokenized_stemmed_path = directory + str(\n",
    "        len(excerpts)) + '_tokenized_stemmed.csv'\n",
    "    if not os.path.exists(tokenized_stemmed_path):\n",
    "        excerpts = excerpts.progress_apply(tokenize_and_stem)\n",
    "        print('saved as csv file..')\n",
    "        excerpts.to_csv(tokenized_stemmed_path)\n",
    "    else:\n",
    "        excerpts = pd.read_csv(tokenized_stemmed_path,\n",
    "                               skiprows=1,\n",
    "                               header=None,\n",
    "                               index_col=0,\n",
    "                               squeeze=True)\n",
    "    excerpts = excerpts.fillna('')\n",
    "    print('Done')\n",
    "\n",
    "    # Lets get the number of top words that overlap in each document\n",
    "    if top_n_word_count == True:\n",
    "\n",
    "        # Instantiate the TfidfVectorizer\n",
    "        tfidf = TfidfVectorizer(stop_words='english',\n",
    "                                min_df=3,\n",
    "                                max_features=None,\n",
    "                                ngram_range=(1, 1),\n",
    "                                use_idf=True,\n",
    "                                smooth_idf=True,\n",
    "                                sublinear_tf=True,\n",
    "                                tokenizer=None,\n",
    "                                preprocessor=None)\n",
    "\n",
    "        # get the relevant vectors\n",
    "        def get_tfidf(excerpts):\n",
    "            excerpts_tfidf = tfidf.fit_transform([x for x in excerpts])\n",
    "            return excerpts_tfidf\n",
    "\n",
    "        print('Getting tfidf vectors of cleaned and tokenized text...')\n",
    "        excerpts_tfidf = get_tfidf(excerpts)\n",
    "\n",
    "        feature_array = np.array(tfidf.get_feature_names())\n",
    "        tfidf_sorting = np.argsort(excerpts_tfidf.toarray()).flatten()[::-1]\n",
    "\n",
    "        # Get the top n words from the tfidf vectorizer\n",
    "        def top_n_words(tfidf_sorting):\n",
    "            n = 1510\n",
    "            top_n = feature_array[tfidf_sorting][:n]\n",
    "            top_pop = list()\n",
    "            for i in tqdm(excerpts, colour='green'):\n",
    "                counter = 0\n",
    "                for x in top_n:\n",
    "                    if x in i:\n",
    "                        counter += 1\n",
    "                top_pop.append(counter)\n",
    "            return top_pop\n",
    "\n",
    "        print(\n",
    "            'Retrieving the top 1510 words and counting instances of top words in every document...'\n",
    "        )\n",
    "        df['top_pop'] = top_n_words(tfidf_sorting)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Word Length\n",
    "    def get_word_length(string):\n",
    "        string = string.split()\n",
    "        temp = np.array([len(x) for x in string])\n",
    "        temp = temp.mean()\n",
    "        return temp\n",
    "\n",
    "    def remove_repeating_words(string):\n",
    "        string = string.split()\n",
    "        k = []\n",
    "        for i in (string):\n",
    "            if (string.count(i) > 1 and (i not in k) or string.count(i) == 1):\n",
    "                k.append(i)\n",
    "        return ' '.join(k)\n",
    "\n",
    "    print(\n",
    "        'Keeping only non repeating words in the corpus for gathering statistics...'\n",
    "    )\n",
    "    non_repeating_word_corpus = excerpts.progress_apply(\n",
    "        remove_repeating_words\n",
    "    )  # The excerpts passed here has been cleaned and tokenized and stemmed\n",
    "\n",
    "    # Get sent_lenghts and word_count after getting the non_repeating_word_corpus\n",
    "\n",
    "    print('Getting mean word length of every document...')\n",
    "    df['mean_word_length'] = excerpts.progress_apply(get_word_length)\n",
    "\n",
    "    # sentence lengths: Character count\n",
    "    if get_sent_lengths == True:\n",
    "        sent_lenghts = pd.Series([\n",
    "            len(non_repeating_word_corpus[i])\n",
    "            for i in range(len(non_repeating_word_corpus))\n",
    "        ])\n",
    "        print(\n",
    "            'Getting character counts aka sentence lengths of each document...'\n",
    "        )\n",
    "        df['sent_lengths'] = sent_lenghts\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # word_count\n",
    "    if get_word_lengths == True:\n",
    "        print('Getting word counts for each document')\n",
    "        df['word_count'] = non_repeating_word_corpus.apply(\n",
    "            str.split).apply(len)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Part of speech Magic\n",
    "    if get_pos_counts == True:\n",
    "        pos = {\n",
    "            'verbs': ['VB', 'VBG', 'VBN', 'VBP', 'VBD', 'VBZ'],\n",
    "            'nouns': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "            'adverbs': ['RB', 'RBR', 'RBS'],\n",
    "            'adjectives': ['JJ', 'JJR', 'JJS'],\n",
    "            'pronouns': ['PRP', 'PRP$']\n",
    "        }\n",
    "\n",
    "        def get_counts(x='list of sentences',\n",
    "                       y='list of pos to get counts',\n",
    "                       repeating_words='Yes',\n",
    "                       correlation_=False,\n",
    "                       add_to_dataframe=True,\n",
    "                       temp_tagged_texts=[]):\n",
    "            if not bool(temp_tagged_texts) == True:\n",
    "                if repeating_words == 'Yes':\n",
    "                    for_tagging = clean_text(x)\n",
    "                    for_tagging = for_tagging.apply(str.split)\n",
    "                    for_tagging = for_tagging.to_list()\n",
    "                elif repeating_words == 'No':\n",
    "                    remove_repeating_words_path = directory + str(\n",
    "                        len(excerpts)) + '_repeating_words_removed.csv'\n",
    "                    if not os.path.exists(remove_repeating_words_path):\n",
    "                        for_tagging = x.progress_apply(remove_repeating_words)\n",
    "                        print('saved as csv file..')\n",
    "                        for_tagging.to_csv(remove_repeating_words_path)\n",
    "                        for_tagging = for_tagging.apply(str.split)\n",
    "                        for_tagging = for_tagging.to_list()\n",
    "                    else:\n",
    "                        for_tagging = pd.read_csv(remove_repeating_words_path,\n",
    "                                                  skiprows=1,\n",
    "                                                  header=None,\n",
    "                                                  index_col=0,\n",
    "                                                  squeeze=True)\n",
    "                        for_tagging = for_tagging.apply(str.split)\n",
    "                        for_tagging = for_tagging.to_list()\n",
    "                print('Tagging parts of speech...')\n",
    "                temp_tagged_texts = pos_tag_sents(for_tagging)\n",
    "            else:\n",
    "                pass\n",
    "            pos_list = []\n",
    "            for i in tqdm(range(len(temp_tagged_texts))):\n",
    "                a, b = zip(*temp_tagged_texts[i])\n",
    "                pos_list.append(list(b))\n",
    "            num_pos = list()\n",
    "            for i in tqdm(pos_list):\n",
    "                cnt = Counter(i)\n",
    "                z = 0\n",
    "                for j in y:\n",
    "                    z += cnt[j]\n",
    "                num_pos.append(z)\n",
    "            if add_to_dataframe == True:\n",
    "                df_name = 'num_' + p\n",
    "                df[str(df_name)] = num_pos\n",
    "            elif add_to_dataframe == False:\n",
    "                return np.corrcoef(num_pos, df['target_column'])\n",
    "            return temp_tagged_texts\n",
    "\n",
    "        print('Getting parts of speech counts of: ', list_of_pos)\n",
    "        tagged_texts = []\n",
    "        for ind, p in enumerate(list_of_pos):\n",
    "            print('Getting counts of all ', p, '...')\n",
    "            if ind == 0:\n",
    "                tagged_texts.append(\n",
    "                    get_counts(x=df[text_column],\n",
    "                               y=pos[p],\n",
    "                               repeating_words='No',\n",
    "                               correlation_=False,\n",
    "                               add_to_dataframe=True,\n",
    "                               temp_tagged_texts=tagged_texts))\n",
    "            if ind > 0:\n",
    "                tagged_texts.append(\n",
    "                    get_counts(x=df[text_column],\n",
    "                               y=pos[p],\n",
    "                               repeating_words='No',\n",
    "                               correlation_=False,\n",
    "                               add_to_dataframe=True,\n",
    "                               temp_tagged_texts=tagged_texts[0]))\n",
    "        print('All necessary parts of speech counts have been processed')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print('Done')\n",
    "    print('Getting Embeddings')\n",
    "    sentences = {'df': df.excerpt}\n",
    "\n",
    "    model = SentenceTransformer(model_path)\n",
    "\n",
    "    def get_embeddings(x):\n",
    "        embedding_cache_path = directory + x + str(len(\n",
    "            df.excerpt)) + '-embeddings-' + model_name.replace('/',\n",
    "                                                               '_') + '.pkl'\n",
    "        text = sentences[x]\n",
    "        if not os.path.exists(embedding_cache_path):\n",
    "            embeddings = []\n",
    "            print('Extracting numerical representations for all documents')\n",
    "            for i in tqdm(range(len(text))):\n",
    "                corpus_embed = model.encode(text[i])\n",
    "                embeddings.append(corpus_embed)\n",
    "\n",
    "            print('Storing file on disc')\n",
    "            with open(embedding_cache_path, 'wb') as fOut:\n",
    "                pickle.dump({'text': text, 'embeddings': embeddings}, fOut)\n",
    "                print('Done')\n",
    "        else:\n",
    "            print(\"Loading pre-computed embeddings from disc\")\n",
    "            with open(embedding_cache_path, 'rb') as fIn:\n",
    "                cache_data = pickle.load(fIn)\n",
    "                corpus_sentences = cache_data['text']\n",
    "                embeddings = cache_data['embeddings']\n",
    "            print('Done')\n",
    "        return embeddings\n",
    "\n",
    "    df_embeddings = pd.DataFrame(get_embeddings('df'))\n",
    "    final_df = pd.concat([\n",
    "        df_embeddings, df[[\n",
    "            'top_pop', 'mean_word_length', 'sent_lengths', 'word_count',\n",
    "            'num_nouns', 'num_verbs', 'num_adjectives'\n",
    "        ]]\n",
    "    ],\n",
    "                         axis=1)\n",
    "    final_df.to_csv(str(len(final_df)) + model_name +\n",
    "                    '_embedding_features.csv.gz',\n",
    "                    compression='gzip')\n",
    "    print('Final dataframe has been saved!')\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415378f",
   "metadata": {},
   "source": [
    "# Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "902acba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:29:39.448337Z",
     "start_time": "2021-09-17T23:29:39.439334Z"
    }
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def nn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(512, input_dim=775, kernel_initializer='normal',\n",
    "              activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='normal',\n",
    "              activation='relu'))\n",
    "    model.add(Dense(128, kernel_initializer='normal',\n",
    "              activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='normal',\n",
    "              activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics = [keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4339e3",
   "metadata": {},
   "source": [
    "# Train the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8eb0b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T20:38:18.463821Z",
     "start_time": "2021-09-17T20:34:31.091792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5d840b4f064442b163cbbf7ca3f5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce6bcb663734403b03cb75b9ea1ce66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf325e8db95433db2608f13d4c17f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39498a894d644eab07bab95f064adc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f343003608764dfb9f0f9deb2f0f1ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0543685a624a2aa949fe6f7b8873b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca4c188cd01480abdc4fe666ba660f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5499d211572400ca4b1acaa2da82e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34669d0ca2e84adeb4b7f1ee2849358e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n",
      "Epoch 1/100\n",
      "567/567 [==============================] - 17s 3ms/step - loss: 0.9620 - root_mean_squared_error: 0.9581\n",
      "Epoch 2/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.4016 - root_mean_squared_error: 0.6332\n",
      "Epoch 3/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3545 - root_mean_squared_error: 0.5952\n",
      "Epoch 4/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3481 - root_mean_squared_error: 0.5897\n",
      "Epoch 5/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3099 - root_mean_squared_error: 0.5566\n",
      "Epoch 6/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2929 - root_mean_squared_error: 0.5410\n",
      "Epoch 7/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2883 - root_mean_squared_error: 0.5369\n",
      "Epoch 8/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2507 - root_mean_squared_error: 0.5006\n",
      "Epoch 9/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2292 - root_mean_squared_error: 0.4785\n",
      "Epoch 10/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2274 - root_mean_squared_error: 0.4765\n",
      "Epoch 11/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1949 - root_mean_squared_error: 0.4413\n",
      "Epoch 12/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1778 - root_mean_squared_error: 0.4214\n",
      "Epoch 13/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1688 - root_mean_squared_error: 0.4106\n",
      "Epoch 14/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1598 - root_mean_squared_error: 0.3996\n",
      "Epoch 15/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1300 - root_mean_squared_error: 0.3604\n",
      "Epoch 16/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1268 - root_mean_squared_error: 0.3559\n",
      "Epoch 17/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1125 - root_mean_squared_error: 0.3352\n",
      "Epoch 18/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1164 - root_mean_squared_error: 0.3411\n",
      "Epoch 19/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0914 - root_mean_squared_error: 0.3021\n",
      "Epoch 20/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0829 - root_mean_squared_error: 0.2877\n",
      "Epoch 21/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0851 - root_mean_squared_error: 0.2915\n",
      "Epoch 22/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0807 - root_mean_squared_error: 0.2839\n",
      "Epoch 23/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0635 - root_mean_squared_error: 0.2518\n",
      "Epoch 24/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0564 - root_mean_squared_error: 0.2373\n",
      "Epoch 25/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0605 - root_mean_squared_error: 0.2458\n",
      "Epoch 26/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0558 - root_mean_squared_error: 0.2362\n",
      "Epoch 27/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0526 - root_mean_squared_error: 0.2290\n",
      "Epoch 28/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0531 - root_mean_squared_error: 0.2299\n",
      "Epoch 29/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0477 - root_mean_squared_error: 0.2182\n",
      "Epoch 30/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0428 - root_mean_squared_error: 0.2068\n",
      "Epoch 31/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0389 - root_mean_squared_error: 0.1966\n",
      "Epoch 32/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0407 - root_mean_squared_error: 0.2015\n",
      "Epoch 33/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0425 - root_mean_squared_error: 0.2061\n",
      "Epoch 34/100\n",
      "567/567 [==============================] - ETA: 0s - loss: 0.0427 - root_mean_squared_error: 0.206 - 2s 3ms/step - loss: 0.0427 - root_mean_squared_error: 0.2064\n",
      "Epoch 35/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0386 - root_mean_squared_error: 0.1964\n",
      "Epoch 36/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0334 - root_mean_squared_error: 0.1826\n",
      "Epoch 37/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0373 - root_mean_squared_error: 0.1930A: 0s - loss: 0.0375 - root_mean_squared_error: 0\n",
      "Epoch 38/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0326 - root_mean_squared_error: 0.1806\n",
      "Epoch 39/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0307 - root_mean_squared_error: 0.1751\n",
      "Epoch 40/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0257 - root_mean_squared_error: 0.1601\n",
      "Epoch 41/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0320 - root_mean_squared_error: 0.1784\n",
      "Epoch 42/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0230 - root_mean_squared_error: 0.1516\n",
      "Epoch 43/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0252 - root_mean_squared_error: 0.1585\n",
      "Epoch 44/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0221 - root_mean_squared_error: 0.1483\n",
      "Epoch 45/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0292 - root_mean_squared_error: 0.1708\n",
      "Epoch 46/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0262 - root_mean_squared_error: 0.1615\n",
      "Epoch 47/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0226 - root_mean_squared_error: 0.1501\n",
      "Epoch 48/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0212 - root_mean_squared_error: 0.1455\n",
      "Epoch 49/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0219 - root_mean_squared_error: 0.1480\n",
      "Epoch 50/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0214 - root_mean_squared_error: 0.1455\n",
      "Epoch 51/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0211 - root_mean_squared_error: 0.1453\n",
      "Epoch 52/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0174 - root_mean_squared_error: 0.1320\n",
      "Epoch 53/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0216 - root_mean_squared_error: 0.1464\n",
      "Epoch 54/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0298 - root_mean_squared_error: 0.1727\n",
      "Epoch 55/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0241 - root_mean_squared_error: 0.1550\n",
      "Epoch 56/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0167 - root_mean_squared_error: 0.1293\n",
      "Epoch 57/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1172\n",
      "Epoch 58/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0168 - root_mean_squared_error: 0.1294\n",
      "Epoch 59/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0229 - root_mean_squared_error: 0.1509\n",
      "Epoch 60/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0246 - root_mean_squared_error: 0.1563\n",
      "Epoch 61/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0186 - root_mean_squared_error: 0.1362\n",
      "Epoch 62/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0219 - root_mean_squared_error: 0.1480\n",
      "Epoch 63/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1212\n",
      "Epoch 64/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0160 - root_mean_squared_error: 0.1265\n",
      "Epoch 65/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0176 - root_mean_squared_error: 0.1326\n",
      "Epoch 66/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1159\n",
      "Epoch 67/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1235\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0156 - root_mean_squared_error: 0.1245\n",
      "Epoch 69/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0341 - root_mean_squared_error: 0.1831\n",
      "Epoch 70/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0156 - root_mean_squared_error: 0.1247\n",
      "Epoch 71/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0165 - root_mean_squared_error: 0.1281\n",
      "Epoch 72/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0107 - root_mean_squared_error: 0.1036\n",
      "Epoch 73/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1207\n",
      "Epoch 74/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0170 - root_mean_squared_error: 0.1294\n",
      "Epoch 75/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0108 - root_mean_squared_error: 0.1041\n",
      "Epoch 76/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1204\n",
      "Epoch 77/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0116 - root_mean_squared_error: 0.1075\n",
      "Epoch 78/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1170\n",
      "Epoch 79/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0268 - root_mean_squared_error: 0.1633\n",
      "Epoch 80/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1214\n",
      "Epoch 81/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0108 - root_mean_squared_error: 0.1040\n",
      "Epoch 82/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0079 - root_mean_squared_error: 0.0889\n",
      "Epoch 83/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0086 - root_mean_squared_error: 0.0927\n",
      "Epoch 84/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0166 - root_mean_squared_error: 0.1281\n",
      "Epoch 85/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.0164 - root_mean_squared_error: 0.1278\n",
      "Epoch 86/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0107 - root_mean_squared_error: 0.1034\n",
      "Epoch 87/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0099 - root_mean_squared_error: 0.0996\n",
      "Epoch 88/100\n",
      "567/567 [==============================] - 3s 4ms/step - loss: 0.0120 - root_mean_squared_error: 0.1093\n",
      "Epoch 89/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0154 - root_mean_squared_error: 0.1238\n",
      "Epoch 90/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0100 - root_mean_squared_error: 0.1000\n",
      "Epoch 91/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0088 - root_mean_squared_error: 0.0937\n",
      "Epoch 92/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0107 - root_mean_squared_error: 0.1030\n",
      "Epoch 93/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0195 - root_mean_squared_error: 0.1381\n",
      "Epoch 94/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0129 - root_mean_squared_error: 0.1136\n",
      "Epoch 95/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0072 - root_mean_squared_error: 0.0847\n",
      "Epoch 96/100\n",
      "567/567 [==============================] - 1s 3ms/step - loss: 0.0079 - root_mean_squared_error: 0.0889\n",
      "Epoch 97/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0082 - root_mean_squared_error: 0.0905\n",
      "Epoch 98/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0092 - root_mean_squared_error: 0.0955\n",
      "Epoch 99/100\n",
      "567/567 [==============================] - 1s 3ms/step - loss: 0.0117 - root_mean_squared_error: 0.1075\n",
      "Epoch 100/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0141 - root_mean_squared_error: 0.1188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_transformer',\n",
       "                 FunctionTransformer(func=<function feature_engineering at 0x0000019C18755280>)),\n",
       "                ('model',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x0000019C26AE2A60>)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the stages of the pipeline\n",
    "pipeline = Pipeline(steps= [('feature_transformer', FunctionTransformer(feature_engineering)),\n",
    "                            ('model', KerasRegressor(build_fn=nn_model, epochs=100, batch_size=5, verbose=1))])\n",
    "\n",
    "# fit the pipeline model with the training data                            \n",
    "pipeline.fit(train, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88125c19",
   "metadata": {},
   "source": [
    "# Prediction using NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8433fa7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T20:44:09.186988Z",
     "start_time": "2021-09-17T20:44:07.569410Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26554fdf0e1b496189bc6faa93f4e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50264f0797246848dad19d90e5c9424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8489dde5f1bd4573a22ac5be94a2863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0c79fdd6af4002bb8a50acbcfd8a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfffab5ac73645b88b0047746883ee8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa46ee8a37c4130ae8223462a733a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca38048d60ba4589876043c521fc0e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2167c747be046608ee55fbae7ca0cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a6a45efb60481e81dd5f6f10c52371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.6842165 , -0.3029627 ,  0.01700673, -1.8965361 , -2.0734403 ,\n",
       "       -1.1195071 ,  0.4308682 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae93d5",
   "metadata": {},
   "source": [
    "# Define Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6c8cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T20:53:46.810177Z",
     "start_time": "2021-09-17T20:53:10.518036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8badf640cae04f0ab3d123280f71b194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19282d1607cc4976a26657508174db8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef61a4db8fc5491282c942accc917e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f103092c5649e6a3c3f30b941deed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b439d82d9194bce86165195a857d5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc3b8a5e70640de8e0ab5b3012ae770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa01ff288fe4746a2adec2e19a3196e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815a8dfe0d6e49d39a0bdd74c5825021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4094ed55cbf4e0d918647c6f6fe3557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_transformer',\n",
       "                 FunctionTransformer(func=<function feature_engineering at 0x0000019C18755280>)),\n",
       "                ('reg', LinearRegression())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the stages of the pipeline\n",
    "pipeline = Pipeline(steps= [('feature_transformer', FunctionTransformer(feature_engineering)),\n",
    "                            ('reg', LinearRegression())])\n",
    "\n",
    "# fit the pipeline model with the training data                            \n",
    "pipeline.fit(train, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc126f4d",
   "metadata": {},
   "source": [
    "# Predict using Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29de8df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T20:54:01.776865Z",
     "start_time": "2021-09-17T20:54:00.348124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6fe38ce1e646f386c35df52f050680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d2f46e470f46c4875b102159ea439f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f5b962d20c47ca9e71b70d026c8739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a948e3675f54e169e28b89af45fd65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9186b0d25e944af86834e86915b59cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ff9cd06de941218c1bb239b839a47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7fb3ee004643a8a32aaf25dc3d1c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667820af0f124d888d848cfbb0169545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a392739e62e64d978c08f0d8a0d48673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.49421727,  0.49148817, -0.44848235, -1.86980539, -2.16281963,\n",
       "       -1.59148693,  0.42663936])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e94c3f",
   "metadata": {},
   "source": [
    "# Define Custom Voting Regressor with minimum value aggregation method instead of the usual average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71a0c471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:33:27.431817Z",
     "start_time": "2021-09-17T23:29:55.042274Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd0c4860bd14941b2f72f6deb58fc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846530a3d8cf4c55a1fc8287492ede70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04099f30f50e4f7aa18a6d43ba0c4728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be0bfc7cd554645a94cba1add67682f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e902639440c42829bd5308808b42540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41033458adb24eb2bd1d9e5a1e0c73dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c6142806bc4a1586d98371924e44c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f23d844641481192b3bbe64569d375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc77fb3a4c69490c900489a00eeb3bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n",
      "Epoch 1/100\n",
      "567/567 [==============================] - 3s 3ms/step - loss: 0.7278 - root_mean_squared_error: 0.8417\n",
      "Epoch 2/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.4198 - root_mean_squared_error: 0.6475\n",
      "Epoch 3/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3698 - root_mean_squared_error: 0.6079\n",
      "Epoch 4/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3232 - root_mean_squared_error: 0.5684\n",
      "Epoch 5/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3155 - root_mean_squared_error: 0.5616\n",
      "Epoch 6/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.3030 - root_mean_squared_error: 0.5502\n",
      "Epoch 7/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2506 - root_mean_squared_error: 0.5001\n",
      "Epoch 8/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2315 - root_mean_squared_error: 0.4804\n",
      "Epoch 9/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2506 - root_mean_squared_error: 0.4999\n",
      "Epoch 10/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2173 - root_mean_squared_error: 0.4660\n",
      "Epoch 11/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.2189 - root_mean_squared_error: 0.4677\n",
      "Epoch 12/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1978 - root_mean_squared_error: 0.4445\n",
      "Epoch 13/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1611 - root_mean_squared_error: 0.4009\n",
      "Epoch 14/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1640 - root_mean_squared_error: 0.4046\n",
      "Epoch 15/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1397 - root_mean_squared_error: 0.3736\n",
      "Epoch 16/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1292 - root_mean_squared_error: 0.3589\n",
      "Epoch 17/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1145 - root_mean_squared_error: 0.3378\n",
      "Epoch 18/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.1044 - root_mean_squared_error: 0.3229\n",
      "Epoch 19/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0935 - root_mean_squared_error: 0.3056\n",
      "Epoch 20/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0910 - root_mean_squared_error: 0.3015\n",
      "Epoch 21/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0694 - root_mean_squared_error: 0.2631\n",
      "Epoch 22/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0766 - root_mean_squared_error: 0.2766\n",
      "Epoch 23/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0639 - root_mean_squared_error: 0.2527\n",
      "Epoch 24/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0613 - root_mean_squared_error: 0.2475\n",
      "Epoch 25/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0567 - root_mean_squared_error: 0.2376\n",
      "Epoch 26/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0607 - root_mean_squared_error: 0.2459\n",
      "Epoch 27/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0480 - root_mean_squared_error: 0.2189\n",
      "Epoch 28/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0486 - root_mean_squared_error: 0.2203\n",
      "Epoch 29/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0428 - root_mean_squared_error: 0.2067\n",
      "Epoch 30/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0523 - root_mean_squared_error: 0.2279A: 0s - loss: 0.0523 - root_mean_squared_error: 0.227\n",
      "Epoch 31/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0421 - root_mean_squared_error: 0.2050\n",
      "Epoch 32/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0582 - root_mean_squared_error: 0.2405\n",
      "Epoch 33/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0383 - root_mean_squared_error: 0.1956\n",
      "Epoch 34/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0350 - root_mean_squared_error: 0.1865\n",
      "Epoch 35/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0365 - root_mean_squared_error: 0.1909\n",
      "Epoch 36/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0361 - root_mean_squared_error: 0.1898\n",
      "Epoch 37/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0353 - root_mean_squared_error: 0.1876\n",
      "Epoch 38/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0272 - root_mean_squared_error: 0.1649\n",
      "Epoch 39/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0301 - root_mean_squared_error: 0.1731\n",
      "Epoch 40/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0303 - root_mean_squared_error: 0.1740\n",
      "Epoch 41/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0449 - root_mean_squared_error: 0.2110\n",
      "Epoch 42/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0210 - root_mean_squared_error: 0.1447\n",
      "Epoch 43/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0242 - root_mean_squared_error: 0.1555\n",
      "Epoch 44/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0241 - root_mean_squared_error: 0.1549\n",
      "Epoch 45/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0402 - root_mean_squared_error: 0.2000\n",
      "Epoch 46/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0251 - root_mean_squared_error: 0.1585\n",
      "Epoch 47/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0259 - root_mean_squared_error: 0.1608\n",
      "Epoch 48/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0184 - root_mean_squared_error: 0.1355\n",
      "Epoch 49/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0174 - root_mean_squared_error: 0.1317\n",
      "Epoch 50/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0268 - root_mean_squared_error: 0.1635\n",
      "Epoch 51/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0197 - root_mean_squared_error: 0.1403\n",
      "Epoch 52/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0169 - root_mean_squared_error: 0.1300\n",
      "Epoch 53/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0186 - root_mean_squared_error: 0.1365\n",
      "Epoch 54/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0183 - root_mean_squared_error: 0.1350\n",
      "Epoch 55/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0184 - root_mean_squared_error: 0.1357\n",
      "Epoch 56/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0211 - root_mean_squared_error: 0.1445\n",
      "Epoch 57/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0176 - root_mean_squared_error: 0.1326\n",
      "Epoch 58/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0372 - root_mean_squared_error: 0.1923\n",
      "Epoch 59/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0162 - root_mean_squared_error: 0.1273\n",
      "Epoch 60/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0163 - root_mean_squared_error: 0.1276\n",
      "Epoch 61/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0161 - root_mean_squared_error: 0.1268\n",
      "Epoch 62/100\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.0239 - root_mean_squared_error: 0.1546\n",
      "Epoch 63/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0202 - root_mean_squared_error: 0.1419\n",
      "Epoch 64/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1198\n",
      "Epoch 65/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0136 - root_mean_squared_error: 0.1164\n",
      "Epoch 66/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167\n",
      "Epoch 67/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0188 - root_mean_squared_error: 0.1368\n",
      "Epoch 68/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0157 - root_mean_squared_error: 0.1252\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0131 - root_mean_squared_error: 0.1145\n",
      "Epoch 70/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0175 - root_mean_squared_error: 0.1318\n",
      "Epoch 71/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194\n",
      "Epoch 72/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0234 - root_mean_squared_error: 0.1526\n",
      "Epoch 73/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0161 - root_mean_squared_error: 0.1265\n",
      "Epoch 74/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0101 - root_mean_squared_error: 0.1004\n",
      "Epoch 75/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0114 - root_mean_squared_error: 0.1065\n",
      "Epoch 76/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1209\n",
      "Epoch 77/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0113 - root_mean_squared_error: 0.1061\n",
      "Epoch 78/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0118 - root_mean_squared_error: 0.1087\n",
      "Epoch 79/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0125 - root_mean_squared_error: 0.1117\n",
      "Epoch 80/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0169 - root_mean_squared_error: 0.1295\n",
      "Epoch 81/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0128 - root_mean_squared_error: 0.1127\n",
      "Epoch 82/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0123 - root_mean_squared_error: 0.1109\n",
      "Epoch 83/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0117 - root_mean_squared_error: 0.1081\n",
      "Epoch 84/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0094 - root_mean_squared_error: 0.0968\n",
      "Epoch 85/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0106 - root_mean_squared_error: 0.1030\n",
      "Epoch 86/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0155 - root_mean_squared_error: 0.1243\n",
      "Epoch 87/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0142 - root_mean_squared_error: 0.1190\n",
      "Epoch 88/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0109 - root_mean_squared_error: 0.1041\n",
      "Epoch 89/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0095 - root_mean_squared_error: 0.0975\n",
      "Epoch 90/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0100 - root_mean_squared_error: 0.1001\n",
      "Epoch 91/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0121 - root_mean_squared_error: 0.1098\n",
      "Epoch 92/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0117 - root_mean_squared_error: 0.1080\n",
      "Epoch 93/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0130 - root_mean_squared_error: 0.1138\n",
      "Epoch 94/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0111 - root_mean_squared_error: 0.1051\n",
      "Epoch 95/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0082 - root_mean_squared_error: 0.0905\n",
      "Epoch 96/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0157 - root_mean_squared_error: 0.1249\n",
      "Epoch 97/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0111 - root_mean_squared_error: 0.1054\n",
      "Epoch 98/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0082 - root_mean_squared_error: 0.0908\n",
      "Epoch 99/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0081 - root_mean_squared_error: 0.0902\n",
      "Epoch 100/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.0093 - root_mean_squared_error: 0.0965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_transformer',\n",
       "                 FunctionTransformer(func=<function feature_engineering at 0x0000021F0CE20C10>)),\n",
       "                ('model',\n",
       "                 CustomMetaRegressor(estimators=[('LinearRegression',\n",
       "                                                  LinearRegression()),\n",
       "                                                 ('NeuralNetwork',\n",
       "                                                  <keras.wrappers.scikit_learn.KerasRegressor object at 0x0000021F0CDBA520>)]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the stages of the pipeline\n",
    "keras_model = KerasRegressor(build_fn=nn_model,\n",
    "                             epochs=100,\n",
    "                             batch_size=5,\n",
    "                             verbose=1)\n",
    "keras_model._estimator_type = \"regressor\"\n",
    "\n",
    "class CustomMetaRegressor(VotingRegressor):\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "        Returns\n",
    "        ----------\n",
    "        maj : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        return np.min(self._predict(X), axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[('feature_transformer', FunctionTransformer(feature_engineering)),\n",
    "           ('model',\n",
    "            CustomMetaRegressor([(\n",
    "                'LinearRegression',\n",
    "                LinearRegression()), ('NeuralNetwork', keras_model)]))])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipeline.fit(train, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa8d1a",
   "metadata": {},
   "source": [
    "# Predict using CustomMetaRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46ce0ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T23:33:37.900293Z",
     "start_time": "2021-09-17T23:33:36.345429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775c13f8886142b6a94ff84a232dfa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5a66a722524c6fbace00dd7b5208ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e767d3c8c84bcb8782f1e0846e6dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7dd37054ac421980eb135ca2fadd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f736221b19234ce9b23f7923a4d142c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc985e0327a434c9c389f787f0cfad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8988b9a0304b80a6b757e3a0c92acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5025e82a896a48ebbfe2abe61814ef0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114d408c3c6c4222a331440843a98676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F0D113310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.49421727, -0.31659186, -0.44848235, -2.17845273, -2.16281963,\n",
       "       -1.59148693,  0.25276405])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb29803",
   "metadata": {},
   "source": [
    "# Save model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4d57044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T01:28:40.482956Z",
     "start_time": "2021-09-18T01:28:40.399153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the Keras model first:\n",
    "pipeline.named_steps.model.named_estimators_.NeuralNetwork.model.save(directory + 'keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf6a9698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T01:31:08.305962Z",
     "start_time": "2021-09-18T01:31:08.293964Z"
    }
   },
   "outputs": [],
   "source": [
    "# This hack allows us to save the sklearn pipeline:\n",
    "pipeline.named_steps.model.named_estimators_.NeuralNetwork.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7c031ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T01:31:49.532179Z",
     "start_time": "2021-09-18T01:31:49.518216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\abhil\\\\Documents\\\\DC\\\\Projects\\\\CommonLit Readability Prize\\\\deploy\\\\sklearn_pipeline.pkl']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, save the pipeline:\n",
    "joblib.dump(pipeline, str(directory + 'sklearn_pipeline.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2e9ff",
   "metadata": {},
   "source": [
    "# Reconstruct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d800977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T01:32:57.411535Z",
     "start_time": "2021-09-18T01:32:55.041044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Text has been cleaned\n",
      "Step 2: Executing the tokenizer and stemmer...might take a while..sit tight...\n",
      "Done\n",
      "Getting tfidf vectors of cleaned and tokenized text...\n",
      "Retrieving the top 1510 words and counting instances of top words in every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cf60bed0b143d586ca39f8eea84d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only non repeating words in the corpus for gathering statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae3edc31dbc4bd0b8e252f1b908acf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting mean word length of every document...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053334cc5ac7494e9ee9b5b614b92b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character counts aka sentence lengths of each document...\n",
      "Getting word counts for each document\n",
      "Getting parts of speech counts of:  ['nouns', 'verbs', 'adjectives']\n",
      "Getting counts of all  nouns ...\n",
      "Tagging parts of speech...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c173d0fd6f7345ce985ee1a44c20ce97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cd5f205ee843649c412e40dabfbdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  verbs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be6334f9f1d4ff2bc8976cb7e3560ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402f7519b4b84bc7a3fdd44ca12d8ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting counts of all  adjectives ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1452eb34da48eabed604df1a0c0460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800c44d0ebe547a494c6162d60f44af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary parts of speech counts have been processed\n",
      "Done\n",
      "Getting Embeddings\n",
      "Loading pre-computed embeddings from disc\n",
      "Done\n",
      "Final dataframe has been saved!\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.49421727, -0.31659186, -0.44848235, -2.17845273, -2.16281963,\n",
       "       -1.59148693,  0.25276405])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pipeline first:\n",
    "pipeline = joblib.load(directory + 'sklearn_pipeline.pkl')\n",
    "\n",
    "# Then, load the Keras model:\n",
    "pipeline.named_steps.model.named_estimators_.NeuralNetwork.model = load_model(directory + 'keras_model.h5')\n",
    "\n",
    "pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81726cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
